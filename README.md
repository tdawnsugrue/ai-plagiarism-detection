# Python Files overview

### flatten-json.py  

Flattens a 2D array of `dict` objects and converts to 1D

### get-search-results.py

Takes a file containing atire search results and prints the first hit (both from ID and Name. Based on preliminary tests, `<name>` appears to be the correct tag to use)

### grab-prompt.py

Grabs a random prompt from the corpus of original prompts and formats it as a query for search with ATIRE. 

TODO: alter this one to work with the paraphrased format

### paraphrase_llama.py

takes the entire file of prompts generated by llama (2D ver) and runs them through dipper.

**Problems:**  
Runs through the whole dataset with no reporting whatsoever.  
Only writes when finished with the entire dataset... may take up to 2 weeks to run. Consider running on a smaller dataset or allocating more cpu/gpu.


### paraphrase_minimal.py

Slightly modified version of the sample script for dipper.

### reofrmat-for-atire.py

contains functions to format both queries and index (trec format) items for atire.

### grab-search-etc.py

Takes a prompt (or prompts) and uses TF-IDF to extract the top-n terms; then does *something* with them...
Feeds them to atire as a snack? with the goal of pulling up relevant docs.
When this works it allows us to work on the plagiarism side of the problem... because if we can pull up relevant docs we can use analysis tools and get our % similarity/similarities.

## WORKING NOTES

### The pipeline(done locally; needs to be changed for informational)

1. Get dataset from ms-marco. Run llama script (msmarco-chat) as you would llama (put shell script used here)  
2. Paraphrase dataset using dipper. [Put dipper script & corresponding sbatch here]  
3. Reformat and index the set of llama responses in ATIRE  
4. Run the paraphrased json file through `grab-search-etc.py`. [CURRENTLY WORKING ON]  


### Notes

#### Errors etc

- Llama complains about at least one of the inputs from ms-marco. Currently in the process of identifying the specific entry/entries.  
- Sometimes dipper doesn't work. I have the set of prompts where this happens; refer to the section below

##### Short Dipper Outputs
Out of all 101,071 entries:  
- 138 Llama responses are shorter than 50 characters
- 191 Paraphrased responses are shorter than 50 characters
- 115 of the short paraphrased passages are a result of short llama entries. (I suspect the other ~23 short llama entries have corresponding passages that are similar in length)

Based on analysis of these passages (refer to `problematic-dipper-entries.json`), most of these make sense for the queries; they're asking for measurement conversions/simple yes-no answers/etc.

**However**, the instances with long llama responses seem to be an issue on dipper's part. This could be due to how dipper is handling long inputs; sometimes paraphrased responses are seemingly blank or contain only part of the original phrase.

**What to do about it?**  
For the time being, I'm choosing to act on the assumption that most of these short entries are intentional (and thus probably not very interesting for the purposes of what we're doing.) They'll be excluded from the dataset for now; this *does* mean that I probably need to implement some kind of indexing, however...

### Llama Errors

- The error occurs in at least one entry between 20720 and 20740.
- Will run llama on this dataset to figure out exactly what query this is happening on, and the contents of said query

### TODO

- Continue investigating Llama's error:  
    - identify the problematic query and investigate (is the query blank?)
- Get `grab-search-etc.py` working.
    - even if the results are sub-par, I'd like to be able to run the damn thing.
- Implement KL-Divergence to determine query length; additionally, use a set max number of query terms
- Try to tweak BM-25 (get scripts from C?) to improve results by training it on the data
- Get stats for everything


### STATS SECTION:

#### PART A: ATIRE

(For search results I'm checking i -> correct result in top 1; and ii -> correct result in top 10)

- how well does atire perform with default settings + tf-idf ONLY
- how well atire performs with KL-divergence
- the effect of tweaked bm-25 parameters on the effectiveness of retrieval

#### PART B: PLAGIARISM DETECTION

- an overview of ideally more than 1 plagiarism detector  
- for each: % between identical documents (sanity check)  
                % between original and perturbed

**Sherlock**